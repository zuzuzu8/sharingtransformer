%%time

np.random.seed(123)
tf.random.set_seed(123)

(x, t) = load_data(seed=1984)
char_to_id1, id_to_char1 = get_vocab1()
char_to_id2, id_to_char2 = get_vocab2()

indices = np.arange(len(x))
np.random.shuffle(indices)
x = x[indices]
t = t[indices]

(x_train, x_val, x_test) = x[:40000], x[40000:55000], x[55000:57000]
(t_train, t_val, t_test) = t[:40000], t[40000:55000], t[55000:57000]

train_dataloader = DataLoader((x_train, t_train))
val_dataloader = DataLoader((x_val, t_val))
test_dataloader = DataLoader((x_test, t_test), batch_size=1)

depth_x = len(char_to_id1)
depth_t = len(char_to_id2)
model = Transformer(depth_x, depth_t, eN=6, dN=6, h=8, d_model=400, d_ff=1200, maxlen=40)
model.build(input_shape=(1, 40))  #重みの型を定義する
model.load_weights('C:/Users/kazuk/normal_tf.h5')  #モデルの重みをロードする

print('-'*20)
print('model loaded')

criterion = tf.losses.CategoricalCrossentropy()
optimizer = optimizers.Adam(learning_rate=0.00001,
                            beta_1 = 0.9, beta_2 = 0.999, amsgrad=True)
train_loss = metrics.Mean()
val_loss = metrics.Mean()

def compute_loss(t, y):
    return criterion(t, y)

@tf.function
def train_step(x, t, depth_t):
    with tf.GradientTape() as tape:
        preds = model(x, t)
        t = t[:, 1:]
        mask_t = tf.cast(tf.not_equal(t, 0), tf.float32)
        t = tf.one_hot(t, depth=depth_t, dtype=tf.float32)
        t = t * mask_t[:, :, tf.newaxis]
        loss = compute_loss(t, preds)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    train_loss(loss)
    return preds

@tf.function
def val_step(x, t, depth_t):
    preds = model(x, t)
    t = t[:, 1:]
    mask_t = tf.cast(tf.not_equal(t, 0), tf.float32)
    t = tf.one_hot(t, depth = depth_t, dtype = tf.float32)
    t = t * mask_t[:, :, tf.newaxis]
    loss = compute_loss(t, preds)
    val_loss(loss)
    
    return preds

@tf.function
def test_step(x):
    preds = model(x)
    return preds

epochs=10

hypothesis = []
references = []

for epoch in range(epochs):
    
    print('-' * 20)
    print('epoch: {}'.format(epoch+1))
    
    for(x, t) in train_dataloader:
        train_step(x, t, depth_t)
        
    for(x, t) in val_dataloader:
        val_step(x, t, depth_t)
        
    print('loss: {:.3f}, val_loss: {:.3}'.format(
        train_loss.result(),
        val_loss.result()
    ))
    
    for idx, (x, t) in enumerate(test_dataloader):
        outid = []
        outid2 = []
        targetid = []
        preds = test_step(x)
        
        source = x.numpy().reshape(-1)
        target = t.numpy().reshape(-1)
        out = preds.numpy().reshape(-1)
        

        for c in target.flatten():
            if c == 1:
                continue
            if c == 2:
                break
            targetid.append(c)
          
        references.append([id_to_char2[int(c)] for c in targetid])
        for c in out:
            if c == 1:
                continue
            if c == 2:
                break
            outid2.append(c)
          
        hypothesis.append([id_to_char2[int(c)] for c in outid2])
          #hypothesis += [' ']*(len(references)-len(hypothesis))
  
        source = ''.join([id_to_char1[int(c)] for c in source.flatten()])
        target = ''.join([id_to_char2[int(c)] for c in target.flatten()])
        for c in out:
            outid.append(c)
            if c == 2:
                break
          
        out = ''.join([id_to_char2[int(c)] for c in outid])
        
        
        print('>', source)
        print('=', target)
        print('<', out)
        print()
        
        if idx >= 2:
            break

    if (epoch + 1) % 5 == 0:
        my_score = 0  
        for i in range(15):
            my_score += bleu_score(references[i], hypothesis[i])
        my_score /= 15
        print("BLEU: {}".format(my_score))
        references = []
        hypothesis = []

    model.save_weights('./Users/kazuk/normal_tf.h5')
    print('model weights saved to : {}'.format('normal.h5'))
