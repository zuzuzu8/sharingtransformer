class MultiHeadAttention(Layer):
    def __init__(self, h, d_model):
        super().__init__()
        self.h = h
        self.d_model = d_model
        self.d_k = d_k = d_model // h
        self.d_v = d_v = d_model // h
        self.attn = ScaleDotProductAttention(d_k)
        
    def build(self, input_shape):
        self.W_q = self.add_weight(name ='W_q',
                                   shape =(self.h,
                                           self.d_model,
                                           self.d_k),
                                   initializer ='glorot_normal',
                                   trainable =True)
        self.W_k = self.add_weight(name ='W_k',
                                   shape =(self.h,
                                           self.d_model,
                                           self.d_k),
                                   initializer = 'glorot_normal',
                                   trainable =True)
        self.W_v = self.add_weight(name ='W_v',
                                   shape =(self.h,
                                           self.d_model,
                                           self.d_v),
                                   initializer ='glorot_normal',
                                   trainable =True)
        self.W_o = self.add_weight(name ='W_o',
                                   shape =(self.h * self.d_v,
                                           self.d_model),
                                   initializer ='glorot_normal',
                                   trainable =True)
        self.b_o = self.add_weight(name ='b_o',
                                   shape =(self.d_model),
                                   initializer ='zeros',
                                   trainable =True)
        super().build(input_shape)
        
    def call(self, q, k, v, mask=None):
        q = tf.einsum('hijk, hkl ->hijl',
                      tf.tile(q[tf.newaxis, :, :, :],
                              [self.h, 1, 1, 1]),
                      self.W_q)
        k = tf.einsum('hijk, hkl ->hijl',
                      tf.tile(k[tf.newaxis, :, :, :],
                              [self.h, 1, 1, 1]),
                      self.W_k)
        v = tf.einsum('hijk, hkl ->hijl',
                      tf.tile(v[tf.newaxis, :, :, :],
                              [self.h, 1, 1, 1]),
                     self.W_v)
        
        q = tf.reshape(q, shape=(-1, q.shape[-2], q.shape[-1]))
        k = tf.reshape(k, shape=(-1, k.shape[-2], k.shape[-1]))
        v = tf.reshape(v, shape=(-1, v.shape[-2], v.shape[-1]))
        
        if mask is not None:
            multiples = [self.h] + [1] * (len(mask.shape) - 1)
            mask = tf.tile(mask, multiples = multiples)
            
        c = self.attn(q, k, v, mask=mask)
        c =tf.split(c, self.h, axis=0)
        c =tf.concat(c, axis=-1)
        
        out = tf.einsum('ijk, kl ->ijl', c, self.W_o) + self.b_o
        
        return out
        
        #layerの2層目で使用する

class MultiHeadAttentionStart(Layer):
    def __init__(self, h, d_model):
        super().__init__()
        self.h = h
        self.d_model = d_model
        self.d_k = d_k = d_model // h
        self.d_v = d_v = d_model // h
        self.attn = ScaleDotProductAttention(d_k)
        
    def build(self, input_shape):
        self.W_q = self.add_weight(name ='W_q',
                                   shape =(self.h,
                                           self.d_model,
                                           self.d_k),
                                   initializer ='glorot_normal',
                                   trainable =True)
        self.W_k = self.add_weight(name ='W_k',
                                   shape =(self.h,
                                           self.d_model,
                                           self.d_k),
                                   initializer = 'glorot_normal',
                                   trainable =True)
        self.W_v = self.add_weight(name ='W_v',
                                   shape =(self.h,
                                           self.d_model,
                                           self.d_v),
                                   initializer ='glorot_normal',
                                   trainable =True)
        self.W_o = self.add_weight(name ='W_o',
                                   shape =(self.h * self.d_v,
                                           self.d_model),
                                   initializer ='glorot_normal',
                                   trainable =True)
        self.b_o = self.add_weight(name ='b_o',
                                   shape =(self.d_model),
                                   initializer ='zeros',
                                   trainable =True)
        super().build(input_shape)
        
    def call(self, q, k, v, mask=None):
        q = tf.einsum('hijk, hkl ->hijl',
                      tf.tile(q[tf.newaxis, :, :, :],
                              [self.h, 1, 1, 1]),
                      self.W_q)
        k = tf.einsum('hijk, hkl ->hijl',
                      tf.tile(k[tf.newaxis, :, :, :],
                              [self.h, 1, 1, 1]),
                      self.W_k)
        v = tf.einsum('hijk, hkl ->hijl',
                      tf.tile(v[tf.newaxis, :, :, :],
                              [self.h, 1, 1, 1]),
                     self.W_v)
        
        q = tf.reshape(q, shape=(-1, q.shape[-2], q.shape[-1]))
        k = tf.reshape(k, shape=(-1, k.shape[-2], k.shape[-1]))
        v = tf.reshape(v, shape=(-1, v.shape[-2], v.shape[-1]))
        
        if mask is not None:
            multiples = [self.h] + [1] * (len(mask.shape) - 1)
            mask = tf.tile(mask, multiples = multiples)
            
        c = self.attn(q, k, v, mask=mask)
        c =tf.split(c, self.h, axis=0)
        c =tf.concat(c, axis=-1)
        
        out = tf.einsum('ijk, kl ->ijl', c, self.W_o) + self.b_o
        
        return out, self.W_q, self.W_k, self.W_v, self.W_o, self.b_o
        
        #layerの3層目から最後の1つ前の層まで使用する

class MultiHeadAttentionSharing(Layer):
    def __init__(self, h, d_model):
        super().__init__()
        self.h = h
        self.d_model = d_model
        self.d_k = d_k = d_model // h
        self.attn = ScaleDotProductAttention(d_k)
        
    def call(self, q, k, v, Wq, Wk, Wv, Wo, bo, mask=None):
        q = tf.einsum('hijk, hkl ->hijl',
                      tf.tile(q[tf.newaxis, :, :, :],
                              [self.h, 1, 1, 1]),
                      Wq)
        k = tf.einsum('hijk, hkl ->hijl',
                      tf.tile(k[tf.newaxis, :, :, :],
                              [self.h, 1, 1, 1]),
                      Wk)
        v = tf.einsum('hijk, hkl ->hijl',
                      tf.tile(v[tf.newaxis, :, :, :],
                              [self.h, 1, 1, 1]),
                     Wv)
        
        q = tf.reshape(q, shape=(-1, q.shape[-2], q.shape[-1]))
        k = tf.reshape(k, shape=(-1, k.shape[-2], k.shape[-1]))
        v = tf.reshape(v, shape=(-1, v.shape[-2], v.shape[-1]))
        
        if mask is not None:
            multiples = [self.h] + [1] * (len(mask.shape) - 1)
            mask = tf.tile(mask, multiples = multiples)
            
        c = self.attn(q, k, v, mask=mask)
        c =tf.split(c, self.h, axis=0)
        c =tf.concat(c, axis=-1)
        
        out = tf.einsum('ijk, kl ->ijl', c, Wo) + bo
        
        return out, Wq, Wk, Wv, Wo, bo
        
        
